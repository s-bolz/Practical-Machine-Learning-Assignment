# Human Activity Recognition Analysis

This analysis is part of the programming assignment for the MOOC
[Practical Machine Learning](https://www.coursera.org/course/predmachlearn). It
uses the
[Weight Lifting Dataset](http://groupware.les.inf.puc-rio.br/work.jsf?p1=11201)
by E. Velloso, A. Bulling, H. Gellersen, W. Ugulino, and H. Fuks to predict the
quality of barbell lifts. The assignment consists of two parts:

1. this report that details the process of finding a model that predicts the quality of the exercise on a given training data set
2. prediction on a given testing data set of which we do not know the outcome

## Model Building Strategy

The grade of the second part of the assignment is based on the prediction success rate of our model on the testing data set. In order to achieve the highest grade we should find a model that is tuned perfectly on the test set - even if it is overfitted. So we decide against following the standard procedure of exploring only our training data and holding back the testing data until the end where we predict on it only once. On the other hand we still want to reduce overfitting as much as possible. Therefore we follow the following model building strategy:

1. minimal exploratory data analysis on the training and testing data sets
2. splitting of the training data set into a training and a testing partition
3. building several models with cross-validation against the training partition
4. evaluating the models against the testing partition
5. selecting a model based on its purpose

We choose to build our models using the random forests algorithm with 10-fold cross-validation.

## Loading, Subsetting and Splitting the Data

We start with downloading the training and testing data sets and loading both sets into R.

```{r, cache = TRUE}
download.file (
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv",
    "pml-training.csv",
    "curl"
)
download.file (
    "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv",
    "pml-testing.csv",
    "curl"
)
training <- read.csv("pml-training.csv")
testing <- read.csv("pml-testing.csv")
```

## Exploratory Data Analysis

What are the dimensions of both data sets?

```{r}
data.frame (
    trainingDimensions = dim(training),
    testingDimensions  = dim(testing)
)
```

Both sets have the same number of columns. Let's have a look if some of them are different.

```{r}
data.frame (
    notIntesting  = setdiff(names(training), names(testing)),
    notInTraining = setdiff(names(testing), names(training))
)
```

The testing data set has no outcome variable but has an identifier column instead which we already expected. Now let's have a look at NA's. If we find fields that have lots of NA's in the testing data set it would be sensible to exclude these variables from our model.

```{r}
library(plyr)
naProptesting <- sapply(1:dim(testing)[2], function(x){mean(is.na(testing[, x]))})
count(data.frame(naProptesting), "naProptesting")
```

100 variables in the testing data set have no value at all. We don't want them in any of our models so we remove them from our training data set.

```{r}
trainingSubset <- training[, naProptesting < 1]
naPropTraining <- sapply(1:dim(trainingSubset)[2], function(x){mean(is.na(trainingSubset[, x]))})
count(data.frame(naPropTraining), "naPropTraining")
```

Now all our training have no NA's left at all in either data set. Let's look at the first columns of the training data set.

```{r}
trainingSubset[1:10, 1:6]
```

The X column looks to be some kind of identifier or row number. Let's verify this by testing it for uniqueness and its range in both data sets.

```{r}
data.frame (
    data_set = c("training", "testing"),
    unique_x = c(length(unique(training$X)), length(unique(testing$X))),
    min_x    = c(min(unique(training$X)), min(unique(testing$X))),
    max_x    = c(max(unique(training$X)), max(unique(testing$X))),
    rows     = c(dim(training)[1], dim(testing)[1])
)
```

This confirms our assumption so we decide to exclude the variable from all our models as well. But before we do that let's have a look at the user column. We want to know if both data sets cover the same users and how many training data we have per user.

```{r}
trainingUsers <- count(training, "user_name")
names(trainingUsers)[2] <- "training_records"
testingUsers <- count(testing, "user_name")
names(testingUsers)[2] <- "testing_records"
merge(trainingUsers, testingUsers)
```

We have several thousand training observations per user in the testing data set. Using the user name in our model could therefore lead to overfitting. But as described above an overfitted model might be helpful for the second part of the assignment so we decide to keep the user name to include it in at least one of our models. For the same reason we decide to keep the time stamp columns.

```{r}
trainingFinal <- trainingSubset[, -1]
```

## Partitioning of the Training Data Set

Now we split our training data set into a "final" training and a testing data set. We call both sets partitions to emphasize their difference from the original data sets.

```{r}
library(caret)
seed <- 20141119
set.seed(seed)
trainPartInd <- createDataPartition(training$classe, p = 0.7, list = FALSE)
trainPart <- trainingFinal[trainPartInd, ]
testPart <- trainingFinal[-trainPartInd, ]
```

## Model Building

We build all our models with the random forests algorithm and use 10-fold cross-validation for each training. We try out three models that only differ in their predictors:

* _Model 1_: all non-outcome variables
* _Model 2_: all non-outcome variables except user name
* _Model 3_: all non-outcome variables except user name and timestamp/time window variables

```{r, cache = TRUE}
ctrl <- trainControl(method = "cv", number = 10)
set.seed(seed)
model1 <- train (
    classe ~ .,
    data = trainPart,
    method = "rf",
    trControl = ctrl
)
set.seed(seed)
model2 <- train (
    classe ~ . - user_name,
    data = trainPart,
    method = "rf",
    trControl = ctrl
)
set.seed(seed)
model3 <- train (
    classe ~ . - user_name - raw_timestamp_part_1 - raw_timestamp_part_2 - cvtd_timestamp - num_window,
    data = trainPart,
    method = "rf",
    trControl = ctrl
)
```

## Model Evaluation

We start by comparing the model accuracies of each cross-validation sample.

```{r}
library(ggplot2)
sampleAccuracies <- data.frame (
    Model = rep(c("Model 1", "Model 2", "Model 3"), each = 10),
    Resample = c(model1$resample$Resample, model2$resample$Resample, model3$resample$Resample),
    Accuracy = c(model1$resample$Accuracy, model2$resample$Accuracy, model3$resample$Accuracy)
)
ggplot(sampleAccuracies, aes(Resample, Accuracy, colour = Model)) + geom_line(aes(group = Model))
```

The first two models seem to work better than the third model as the highest accuracy of our third model lies below the highest accuracy of our first two models. For folds 6 and 8 the first model is even slightly more accurate than model two, but both models have folds with higher accuracies so that this doesn't matter.

Next let's take a look at the final models and compare their out-of-bag error rates per tree. As all models have been built with the same number of trees (500) per sample we can compare them in the same plot.

```{r}
modelErrors <- data.frame (
    Model = rep(c("Model 1", "Model 2", "Model 3"), each = 500),
    Tree = rep(1:500, 3),
    OOB.Error = c (
        model1$finalModel$err.rate[, 1],
        model2$finalModel$err.rate[, 1],
        model3$finalModel$err.rate[, 1]
    )
)
ggplot(modelErrors, aes(Tree, OOB.Error, colour = Model)) + geom_line()
```

Again the first two models are very close to each other and the third model has a slightly higher error. We will now analyze each model a bit more in depth to understand how well all the trees have been aggregated into a single model.

### Model 1 - Using All Predictors

```{r}
model1; model1$finalModel;
```

With 99.9% our first model has a very high accuracy. Its out-of-bag error rate is 0.07% which is very good as well. However cross-validation and the out-of-bag error rate only help us to reduce bias but do not protect our model from overfitting. In order to analyze its predicting capabilities on a new data set we use it to predict our testing partition and build a confusion matrix on our prediction.

```{r}
confusionMatrix(predict(model1, newdata = testPart), testPart$classe)
```

We still have a very high accuracy of 99,9%. Also the sensitivity and specificity of all classes are very close to 1 as well. Let's have a look at the most important variables in our model.

```{r}
varImp(model1)
```

It is interesting that the user name is not ranked among the 20 most important variables. The most important by far is the raw timestamp part 1 which has the maximum score of 100. It is followed by the window number which still has a very high score with almost 45.

According to the [documentation](http://groupware.les.inf.puc-rio.br/public/papers/2013.Velloso.QAR-WLE.pdf) the features were extracted using a sliding window approach with different lengths from 0.5 to 2.5 seconds. Also the measurements of each exercise were aggregated over the time window. The aggregation features have already been removed from our model when we removed all variables from our training data set that are completely NA in the testing data set. The time window information is still present in the timestamp and window variables. Using them in our first model combined with their extrem high importance makes this model very strongly tuned to the data gathering of our sample data set. Thus we have to assume that our first model is overfitted even though the user name does not contribute very much to it.

### Model 2 - Using All Predictors Except User Name

```{r}
model2; model2$finalModel;
```

Accuracy and out-of-bag error rate are (almost) identical to our first model. We could have expected a close similarity to those values of our first model as we already saw that the user name did not have a strong influence on that model. Let's have a look at the confusion matrix of our testing partition prediction.

```{r}
confusionMatrix(predict(model2, newdata = testPart), testPart$classe)
```

Our second model achieved a slightly higher accuracy on the test set as it had one more correct classification. What about the most important variables?

```{r}
varImp(model2)
```

The top 20 variables are the same as in our first model and even the scores are very similar. Therefore we have to assume overfitting of this model for the same reasons as for our first model.

### Model 3 - Using All Predictors Except User Name and Time Related Variables

```{r}
model3; model3$finalModel;
```

The accuracy of our third model is a bit (0.8%) less than that of our first two models and the out-of-bag error rate is a bit (0.64%) higher. However, both values are still very good - even for the training data. Let's have a look at the accuracy on the testing partition.

```{r}
confusionMatrix(predict(model3, newdata = testPart), testPart$classe)
```

With 31 misclassifications out of the third model has 27 more than our first and 28 more than our second model. Thus its accuracy is about 0.5% lower than that of our first two models. This is great news as the accuracy is still very high whilst this model presumably does not overfit as much as our first two models. Let's look at the most important variables.

```{r}
varImp(model3)
```

This time our model completely relies on measurements that can be taken in a different setup with other or no time windows as well. So we can assume that this model is the least overfitted model and thus works best on completely new data.

## Model Selection

We identified two models that are presumably overfitted and one model that is at least less overfitted. We would thus recommend the third model to be used on new data to predict the exercise quality of any user regardless of the time information. We estimate its error rate on this new data as 1 minus its accuracy on our testing partition which is `r round(1 - confusionMatrix(predict(model3, newdata = testPart), testPart$classe)$overall[1], 4)`.

However, for the second part of this assignment, where we have to classify the `r nrow(testing)` cases from our testing data set we might use model 2 as it has a higher accuracy on our test partition and we assume that the test cases have been split off from the same sample. Before we make a final decision as to which model to use for that data set let's take a look how much the predictions of all our 3 models differ from each other.

```{r}
testingPredictions <- data.frame (
    Problem.ID = testing$problem_id,
    Model1.Predictions = predict(model1, testing),
    Model2.Predictions = predict(model2, testing),
    Model3.Predictions = predict(model3, testing)
)
table(testingPredictions$Model1.Predictions, testingPredictions$Model2.Predictions)
```

The first two models predict exactly the same classes.

```{r}
table(testingPredictions$Model2.Predictions, testingPredictions$Model3.Predictions)
```

The third model predicts exactly the same outcome as the first two models as well. This makes it easy for us, so we can safely use the third model for the second part of the assignment as well and keep our conscience clear as we don't use an obviously overfitted model. Still our third model might still be a bit overfitted, because we built it after doing the exploratory data analysis with the complete data instead of only our training partition. This might led us to some decisions that we would not have done after exploring only the training partition. This is a risk we were willing to take and which cannot be helped now.